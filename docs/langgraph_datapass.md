# åŸºäº LangGraph çš„å¤šæ™ºèƒ½ä½“ SQL ç”Ÿæˆä¸å½’å› åˆ†æç³»ç»Ÿ

## å®Œæ•´å®ç°

```python
from typing import TypedDict, List, Dict, Any, Optional, Annotated
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import pandas as pd
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
import asyncio
from enum import Enum

# ===================== çŠ¶æ€å®šä¹‰ =====================
class AnalysisState(TypedDict):
    """å¤šæ™ºèƒ½ä½“åä½œçŠ¶æ€"""
    # è¾“å…¥ç›¸å…³
    user_query: str                    # ç”¨æˆ·åŸå§‹æŸ¥è¯¢
    business_context: str              # ä¸šåŠ¡èƒŒæ™¯
    time_range: tuple                  # æ—¶é—´èŒƒå›´
    
    # SQLæ™ºèƒ½ä½“è¾“å‡º
    sql_query: str                     # ç”Ÿæˆçš„SQLè¯­å¥
    sql_explanation: str               # SQLè§£é‡Šè¯´æ˜
    query_parameters: Dict[str, Any]   # æŸ¥è¯¢å‚æ•°
    
    # æ•°æ®æŸ¥è¯¢ç»“æœ
    raw_data: List[Dict[str, Any]]     # åŸå§‹æŸ¥è¯¢æ•°æ®
    data_summary: Dict[str, Any]       # æ•°æ®æ‘˜è¦ç»Ÿè®¡
    data_quality_issues: List[str]     # æ•°æ®è´¨é‡é—®é¢˜
    
    # å½’å› åˆ†æç»“æœ
    attribution_results: Dict[str, Any]  # å½’å› åˆ†æç»“æœ
    key_findings: List[str]            # å…³é”®å‘ç°
    recommendations: List[str]         # ä¸šåŠ¡å»ºè®®
    
    # æ‰§è¡ŒçŠ¶æ€
    current_agent: str                 # å½“å‰æ‰§è¡Œæ™ºèƒ½ä½“
    execution_history: List[str]       # æ‰§è¡Œå†å²
    errors: List[str]                  # é”™è¯¯ä¿¡æ¯

# ===================== æ™ºèƒ½ä½“1: SQLç”Ÿæˆä¸æŸ¥è¯¢æ™ºèƒ½ä½“ =====================
class SQLGeneratorAgent:
    """æ™ºèƒ½ä½“1: SQLç”Ÿæˆä¸æ•°æ®æŸ¥è¯¢"""
    
    def __init__(self, db_schema: Dict[str, Any]):
        """
        åˆå§‹åŒ–SQLç”Ÿæˆæ™ºèƒ½ä½“
        
        Args:
            db_schema: æ•°æ®åº“æ¨¡å¼å®šä¹‰
        """
        self.db_schema = db_schema
        self.table_info = self._extract_table_info()
    
    def _extract_table_info(self) -> Dict[str, Any]:
        """æå–æ•°æ®åº“è¡¨ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥è¿æ¥åˆ°å®é™…æ•°æ®åº“è·å–å…ƒæ•°æ®
        # ç®€åŒ–ç¤ºä¾‹ï¼šè¿”å›å›ºå®šçš„è¡¨ç»“æ„
        return {
            "sales": {
                "columns": ["id", "product_id", "sale_date", "amount", "region", "channel"],
                "primary_key": "id",
                "foreign_keys": {"product_id": "products.id"}
            },
            "products": {
                "columns": ["id", "name", "category", "price"],
                "primary_key": "id"
            },
            "users": {
                "columns": ["id", "name", "region", "segment"],
                "primary_key": "id"
            }
        }
    
    def generate_sql(self, state: AnalysisState) -> AnalysisState:
        """ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥"""
        print("ğŸ¤– [SQLç”Ÿæˆæ™ºèƒ½ä½“] å¼€å§‹ç”ŸæˆSQLæŸ¥è¯¢...")
        
        user_query = state["user_query"]
        time_range = state["time_range"]
        business_context = state["business_context"]
        
        # åŸºäºç”¨æˆ·æŸ¥è¯¢è§£ææ„å›¾
        intent = self._parse_user_intent(user_query)
        print(f"    è¯†åˆ«æŸ¥è¯¢æ„å›¾: {intent}")
        
        # æ„å»ºSQLæŸ¥è¯¢
        sql_query = self._build_sql_query(intent, time_range, business_context)
        
        # è§£é‡ŠSQLé€»è¾‘
        explanation = self._explain_sql(sql_query)
        
        # æå–æŸ¥è¯¢å‚æ•°
        params = self._extract_query_parameters(sql_query)
        
        # æ›´æ–°çŠ¶æ€
        return {
            **state,
            "sql_query": sql_query,
            "sql_explanation": explanation,
            "query_parameters": params,
            "current_agent": "sql_generator",
            "execution_history": state["execution_history"] + ["SQLç”Ÿæˆå®Œæˆ"]
        }
    
    def execute_query(self, state: AnalysisState) -> AnalysisState:
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        print("ğŸ¤– [SQLæ‰§è¡Œæ™ºèƒ½ä½“] æ‰§è¡ŒæŸ¥è¯¢å¹¶è·å–æ•°æ®...")
        
        # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ•°æ®åº“æŸ¥è¯¢
        # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿæ•°æ®
        raw_data = self._simulate_database_query(
            state["sql_query"],
            state["query_parameters"]
        )
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        quality_issues = self._check_data_quality(raw_data)
        
        # ç”Ÿæˆæ•°æ®æ‘˜è¦
        summary = self._generate_data_summary(raw_data)
        
        # æ›´æ–°çŠ¶æ€
        return {
            **state,
            "raw_data": raw_data,
            "data_summary": summary,
            "data_quality_issues": quality_issues,
            "current_agent": "sql_executor",
            "execution_history": state["execution_history"] + ["æ•°æ®æŸ¥è¯¢å®Œæˆ"]
        }
    
    def _parse_user_intent(self, query: str) -> Dict[str, Any]:
        """è§£æç”¨æˆ·æŸ¥è¯¢æ„å›¾"""
        # å®é™…é¡¹ç›®ä¸­è¿™é‡Œåº”è¯¥ä½¿ç”¨NLPæ¨¡å‹
        intent_keywords = {
            "é”€å”®": "sales_analysis",
            "ç”¨æˆ·": "user_behavior",
            "äº§å“": "product_performance",
            "è¶‹åŠ¿": "trend_analysis",
            "å¯¹æ¯”": "comparison_analysis"
        }
        
        intent = {
            "type": "sales_analysis",  # é»˜è®¤ç±»å‹
            "metrics": [],
            "dimensions": [],
            "filters": []
        }
        
        # ç®€å•å…³é”®è¯åŒ¹é…
        if "é”€å”®" in query:
            intent["metrics"].append("sales_amount")
        if "ç”¨æˆ·" in query:
            intent["dimensions"].append("user_segment")
        if "äº§å“" in query:
            intent["dimensions"].append("product_category")
        if "è¶‹åŠ¿" in query:
            intent["type"] = "trend_analysis"
        if "å¯¹æ¯”" in query:
            intent["type"] = "comparison_analysis"
            
        return intent
    
    def _build_sql_query(self, intent: Dict, time_range: tuple, context: str) -> str:
        """æ„å»ºSQLæŸ¥è¯¢è¯­å¥"""
        start_date, end_date = time_range
        
        # åŸºäºæ„å›¾æ„å»ºæŸ¥è¯¢
        if intent["type"] == "sales_analysis":
            sql = f"""
            SELECT 
                DATE(s.sale_date) as date,
                p.category as product_category,
                s.region,
                SUM(s.amount) as total_sales,
                COUNT(DISTINCT s.id) as transaction_count,
                AVG(s.amount) as avg_transaction_value
            FROM sales s
            JOIN products p ON s.product_id = p.id
            WHERE s.sale_date BETWEEN '{start_date}' AND '{end_date}'
            GROUP BY DATE(s.sale_date), p.category, s.region
            ORDER BY date DESC, total_sales DESC
            """
        elif intent["type"] == "trend_analysis":
            sql = f"""
            SELECT 
                DATE_TRUNC('week', s.sale_date) as week,
                p.category,
                SUM(s.amount) as weekly_sales,
                LAG(SUM(s.amount)) OVER (PARTITION BY p.category ORDER BY DATE_TRUNC('week', s.sale_date)) as previous_week_sales
            FROM sales s
            JOIN products p ON s.product_id = p.id
            WHERE s.sale_date BETWEEN '{start_date}' AND '{end_date}'
            GROUP BY week, p.category
            ORDER BY week
            """
        else:
            sql = f"""
            SELECT 
                s.*,
                p.name as product_name,
                p.category
            FROM sales s
            JOIN products p ON s.product_id = p.id
            WHERE s.sale_date BETWEEN '{start_date}' AND '{end_date}'
            LIMIT 100
            """
        
        return sql
    
    def _simulate_database_query(self, sql: str, params: Dict) -> List[Dict]:
        """æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢"""
        print(f"    æ¨¡æ‹Ÿæ‰§è¡ŒSQL: {sql[:100]}...")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        dates = pd.date_range(start='2024-01-01', end='2024-03-31', freq='D')
        categories = ['ç”µå­äº§å“', 'å®¶å±…ç”¨å“', 'æœè£…', 'é£Ÿå“']
        regions = ['åä¸œ', 'ååŒ—', 'åå—', 'åä¸­']
        
        data = []
        for i, date in enumerate(dates[:30]):  # ç”Ÿæˆ30å¤©æ•°æ®
            for category in categories:
                for region in regions:
                    # æ¨¡æ‹Ÿé”€å”®æ•°æ®
                    sales_amount = 1000 + (i * 100) + (hash(category) % 500) + (hash(region) % 300)
                    transaction_count = 10 + (i % 5) + (hash(category) % 3)
                    
                    data.append({
                        'date': date.strftime('%Y-%m-%d'),
                        'product_category': category,
                        'region': region,
                        'total_sales': sales_amount,
                        'transaction_count': transaction_count,
                        'avg_transaction_value': sales_amount / max(transaction_count, 1)
                    })
        
        return data
    
    def _check_data_quality(self, data: List[Dict]) -> List[str]:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        issues = []
        
        if not data:
            issues.append("æŸ¥è¯¢ç»“æœä¸ºç©º")
            return issues
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        for i, row in enumerate(data[:10]):  # æ£€æŸ¥å‰10è¡Œ
            for key, value in row.items():
                if value is None:
                    issues.append(f"ç¬¬{i}è¡Œï¼Œåˆ—{key}å­˜åœ¨ç©ºå€¼")
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        categories = set(row.get('product_category') for row in data)
        if len(categories) < 2:
            issues.append("æ•°æ®ç±»åˆ«å•ä¸€ï¼Œå¯èƒ½å½±å“åˆ†æç»“æœ")
        
        return issues
    
    def _generate_data_summary(self, data: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        if not data:
            return {}
        
        df = pd.DataFrame(data)
        
        return {
            "total_rows": len(data),
            "date_range": {
                "min": df['date'].min() if 'date' in df.columns else None,
                "max": df['date'].max() if 'date' in df.columns else None
            },
            "categories": df['product_category'].unique().tolist() if 'product_category' in df.columns else [],
            "regions": df['region'].unique().tolist() if 'region' in df.columns else [],
            "total_sales": df['total_sales'].sum() if 'total_sales' in df.columns else 0,
            "avg_daily_sales": df['total_sales'].mean() if 'total_sales' in df.columns else 0
        }

# ===================== æ™ºèƒ½ä½“2: å½’å› åˆ†ææ™ºèƒ½ä½“ =====================
class AttributionAnalysisAgent:
    """æ™ºèƒ½ä½“2: æ•°æ®å½’å› åˆ†æ"""
    
    def __init__(self, analysis_methods: List[str] = ["shap", "lime", "statistical"]):
        """
        åˆå§‹åŒ–å½’å› åˆ†ææ™ºèƒ½ä½“
        
        Args:
            analysis_methods: å¯ç”¨çš„åˆ†ææ–¹æ³•åˆ—è¡¨
        """
        self.analysis_methods = analysis_methods
        self.method_weights = {
            "shap": 0.4,
            "lime": 0.3,
            "statistical": 0.3
        }
    
    def analyze_attribution(self, state: AnalysisState) -> AnalysisState:
        """æ‰§è¡Œå½’å› åˆ†æ"""
        print("ğŸ” [å½’å› åˆ†ææ™ºèƒ½ä½“] å¼€å§‹åˆ†ææ•°æ®...")
        
        raw_data = state["raw_data"]
        data_summary = state["data_summary"]
        user_query = state["user_query"]
        business_context = state["business_context"]
        
        if not raw_data:
            print("    è­¦å‘Šï¼šæ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
            return {
                **state,
                "attribution_results": {},
                "key_findings": ["æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ"],
                "current_agent": "attribution_analyzer",
                "execution_history": state["execution_history"] + ["å½’å› åˆ†æå®Œæˆï¼ˆæ— æ•°æ®ï¼‰"]
            }
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        df = pd.DataFrame(raw_data)
        
        # æ‰§è¡Œå¤šç»´å½’å› åˆ†æ
        attribution_results = self._perform_multi_dimensional_analysis(df, user_query)
        
        # è¯†åˆ«å…³é”®å‘ç°
        key_findings = self._extract_key_findings(attribution_results, business_context)
        
        # ç”Ÿæˆä¸šåŠ¡å»ºè®®
        recommendations = self._generate_recommendations(key_findings)
        
        # æ›´æ–°çŠ¶æ€
        return {
            **state,
            "attribution_results": attribution_results,
            "key_findings": key_findings,
            "recommendations": recommendations,
            "current_agent": "attribution_analyzer",
            "execution_history": state["execution_history"] + ["å½’å› åˆ†æå®Œæˆ"]
        }
    
    def _perform_multi_dimensional_analysis(self, df: pd.DataFrame, user_query: str) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šç»´å½’å› åˆ†æ"""
        results = {
            "dimension_contribution": {},
            "trend_analysis": {},
            "anomaly_detection": {},
            "correlation_analysis": {}
        }
        
        # 1. ç»´åº¦è´¡çŒ®åº¦åˆ†æ
        if 'product_category' in df.columns and 'total_sales' in df.columns:
            category_contribution = df.groupby('product_category')['total_sales'].agg(['sum', 'mean', 'count']).to_dict()
            results["dimension_contribution"]["by_category"] = category_contribution
        
        if 'region' in df.columns and 'total_sales' in df.columns:
            region_contribution = df.groupby('region')['total_sales'].agg(['sum', 'mean', 'count']).to_dict()
            results["dimension_contribution"]["by_region"] = region_contribution
        
        # 2. è¶‹åŠ¿åˆ†æ
        if 'date' in df.columns and 'total_sales' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            df.set_index('date', inplace=True)
            
            # å‘¨è¶‹åŠ¿
            weekly_trend = df['total_sales'].resample('W').sum().to_dict()
            results["trend_analysis"]["weekly"] = weekly_trend
            
            # ç§»åŠ¨å¹³å‡
            moving_avg = df['total_sales'].rolling(window=7).mean().to_dict()
            results["trend_analysis"]["moving_average_7d"] = moving_avg
        
        # 3. å¼‚å¸¸æ£€æµ‹
        if 'total_sales' in df.columns:
            # ä½¿ç”¨Z-scoreæ£€æµ‹å¼‚å¸¸
            mean_sales = df['total_sales'].mean()
            std_sales = df['total_sales'].std()
            
            anomalies = []
            for idx, row in df.iterrows():
                z_score = abs((row['total_sales'] - mean_sales) / std_sales) if std_sales > 0 else 0
                if z_score > 2:  # é˜ˆå€¼è®¾ä¸º2ä¸ªæ ‡å‡†å·®
                    anomalies.append({
                        'index': idx,
                        'value': row['total_sales'],
                        'z_score': z_score,
                        'date': row.get('date', None),
                        'category': row.get('product_category', None)
                    })
            
            results["anomaly_detection"] = {
                "count": len(anomalies),
                "anomalies": anomalies[:10],  # é™åˆ¶è¿”å›æ•°é‡
                "threshold": 2.0
            }
        
        # 4. ç›¸å…³æ€§åˆ†æ
        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
        if len(numeric_columns) > 1:
            correlation_matrix = df[numeric_columns].corr().to_dict()
            results["correlation_analysis"] = correlation_matrix
        
        return results
    
    def _extract_key_findings(self, attribution_results: Dict, business_context: str) -> List[str]:
        """æå–å…³é”®å‘ç°"""
        findings = []
        
        # åˆ†æç»´åº¦è´¡çŒ®åº¦
        dim_contrib = attribution_results.get("dimension_contribution", {})
        
        if "by_category" in dim_contrib:
            category_sales = dim_contrib["by_category"].get("sum", {})
            if category_sales:
                # æ‰¾å‡ºé”€å”®é¢æœ€é«˜çš„ç±»åˆ«
                max_category = max(category_sales.items(), key=lambda x: x[1])
                min_category = min(category_sales.items(), key=lambda x: x[1])
                
                findings.append(f"é”€å”®é¢æœ€é«˜çš„äº§å“ç±»åˆ«æ˜¯ï¼š{max_category[0]}ï¼Œå æ€»é”€å”®é¢çš„{(max_category[1]/sum(category_sales.values()))*100:.1f}%")
                findings.append(f"é”€å”®é¢æœ€ä½çš„äº§å“ç±»åˆ«æ˜¯ï¼š{min_category[0]}ï¼Œä»…å {(min_category[1]/sum(category_sales.values()))*100:.1f}%")
        
        if "by_region" in dim_contrib:
            region_sales = dim_contrib["by_region"].get("sum", {})
            if region_sales:
                # æ‰¾å‡ºé”€å”®é¢æœ€é«˜çš„åŒºåŸŸ
                max_region = max(region_sales.items(), key=lambda x: x[1])
                findings.append(f"{max_region[0]}åœ°åŒºæ˜¯ä¸»è¦é”€å”®è´¡çŒ®åŒºåŸŸ")
        
        # åˆ†æè¶‹åŠ¿
        trend_analysis = attribution_results.get("trend_analysis", {})
        if "weekly" in trend_analysis:
            weekly_sales = list(trend_analysis["weekly"].values())
            if len(weekly_sales) >= 2:
                growth_rate = ((weekly_sales[-1] - weekly_sales[-2]) / weekly_sales[-2] * 100) if weekly_sales[-2] > 0 else 0
                if growth_rate > 10:
                    findings.append(f"æœ€è¿‘ä¸€å‘¨é”€å”®é¢å¢é•¿æ˜¾è‘—ï¼Œç¯æ¯”å¢é•¿{growth_rate:.1f}%")
                elif growth_rate < -5:
                    findings.append(f"æœ€è¿‘ä¸€å‘¨é”€å”®é¢ä¸‹é™ï¼Œç¯æ¯”ä¸‹é™{abs(growth_rate):.1f}%")
        
        # å¼‚å¸¸æ£€æµ‹ç»“æœ
        anomaly_detection = attribution_results.get("anomaly_detection", {})
        anomaly_count = anomaly_detection.get("count", 0)
        if anomaly_count > 0:
            findings.append(f"æ£€æµ‹åˆ°{anomaly_count}ä¸ªé”€å”®å¼‚å¸¸ç‚¹ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒæŸ¥")
        
        return findings
    
    def _generate_recommendations(self, key_findings: List[str]) -> List[str]:
        """åŸºäºå…³é”®å‘ç°ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        for finding in key_findings:
            if "é”€å”®é¢æœ€é«˜çš„äº§å“ç±»åˆ«" in finding:
                # æå–ç±»åˆ«åç§°
                if "ç”µå­äº§å“" in finding:
                    recommendations.append("å»ºè®®åŠ å¤§å¯¹ç”µå­äº§å“çš„è¥é”€æŠ•å…¥ï¼Œå¯è€ƒè™‘æ†ç»‘é”€å”®æˆ–é™æ—¶æŠ˜æ‰£")
                elif "å®¶å±…ç”¨å“" in finding:
                    recommendations.append("å®¶å±…ç”¨å“éœ€æ±‚ç¨³å®šï¼Œå»ºè®®ä¼˜åŒ–ä¾›åº”é“¾ç®¡ç†ï¼Œç¡®ä¿åº“å­˜å……è¶³")
            
            if "é”€å”®é¢æœ€ä½çš„äº§å“ç±»åˆ«" in finding:
                if "é£Ÿå“" in finding:
                    recommendations.append("é£Ÿå“ç±»åˆ«é”€å”®ä¸ä½³ï¼Œå»ºè®®è¿›è¡Œå¸‚åœºè°ƒç ”ï¼Œè°ƒæ•´äº§å“ç»„åˆæˆ–å®šä»·ç­–ç•¥")
            
            if "åœ°åŒº" in finding and "ä¸»è¦é”€å”®è´¡çŒ®" in finding:
                recommendations.append("å»ºè®®åœ¨ä¸»è¦é”€å”®è´¡çŒ®åœ°åŒºå¢åŠ è¥é”€æ´»åŠ¨å’Œæ¸ é“æŠ•å…¥")
                recommendations.append("å¯è€ƒè™‘å°†æˆåŠŸåœ°åŒºçš„è¥é”€ç­–ç•¥å¤åˆ¶åˆ°å…¶ä»–åœ°åŒº")
            
            if "å¢é•¿æ˜¾è‘—" in finding:
                recommendations.append("è¿‘æœŸå¢é•¿åŠ¿å¤´è‰¯å¥½ï¼Œå»ºè®®åŠ å¤§æŠ•å…¥ï¼Œæ‰©å¤§å¸‚åœºä»½é¢")
            
            if "ä¸‹é™" in finding:
                recommendations.append("é”€å”®é¢å‡ºç°ä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®è¿›è¡Œå¸‚åœºè°ƒç ”ï¼Œäº†è§£å®¢æˆ·éœ€æ±‚å˜åŒ–")
            
            if "å¼‚å¸¸ç‚¹" in finding:
                recommendations.append("å¯¹é”€å”®å¼‚å¸¸ç‚¹è¿›è¡Œæ·±å…¥åˆ†æï¼Œè¯†åˆ«æ˜¯æ•°æ®é—®é¢˜è¿˜æ˜¯ä¸šåŠ¡å¼‚å¸¸")
        
        # é€šç”¨å»ºè®®
        recommendations.append("å»ºè®®å»ºç«‹é”€å”®æ•°æ®ç›‘æ§ä»ªè¡¨æ¿ï¼Œå®æ—¶è·Ÿè¸ªå…³é”®æŒ‡æ ‡")
        recommendations.append("å®šæœŸè¿›è¡Œå½’å› åˆ†æï¼ŒåŠæ—¶è°ƒæ•´ä¸šåŠ¡ç­–ç•¥")
        
        return recommendations

# ===================== æ„å»ºåä½œå›¾ =====================
def create_collaboration_graph() -> StateGraph:
    """åˆ›å»ºå¤šæ™ºèƒ½ä½“åä½œå›¾"""
    
    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    db_schema = {
        "host": "localhost",
        "database": "sales_db",
        "username": "admin"
    }
    sql_agent = SQLGeneratorAgent(db_schema)
    attribution_agent = AttributionAnalysisAgent()
    
    # åˆ›å»ºçŠ¶æ€å›¾
    graph = StateGraph(AnalysisState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("sql_generation", sql_agent.generate_sql)
    graph.add_node("query_execution", sql_agent.execute_query)
    graph.add_node("attribution_analysis", attribution_agent.analyze_attribution)
    
    # è®¾ç½®æ‰§è¡Œæµç¨‹
    graph.set_entry_point("sql_generation")
    graph.add_edge("sql_generation", "query_execution")
    graph.add_edge("query_execution", "attribution_analysis")
    graph.add_edge("attribution_analysis", END)
    
    # å¯é€‰ï¼šæ·»åŠ æ¡ä»¶è¾¹ç”¨äºé”™è¯¯å¤„ç†
    def check_data_quality(state: AnalysisState) -> str:
        """æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œå†³å®šæ˜¯å¦ç»§ç»­"""
        if state["data_quality_issues"] and len(state["data_quality_issues"]) > 3:
            return "needs_data_cleanup"
        return "attribution_analysis"
    
    # æ·»åŠ æ¡ä»¶è·¯ç”±
    graph.add_conditional_edges(
        "query_execution",
        check_data_quality,
        {
            "attribution_analysis": "attribution_analysis",
            "needs_data_cleanup": END  # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥æ·»åŠ æ•°æ®æ¸…æ´—èŠ‚ç‚¹
        }
    )
    
    return graph.compile()

# ===================== ä½¿ç”¨ç¤ºä¾‹ =====================
class MultiAgentCollaborator:
    """å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–åä½œç³»ç»Ÿ"""
        self.graph = create_collaboration_graph()
        self.execution_history = []
    
    def analyze_business_query(self, 
                              user_query: str, 
                              business_context: str = "",
                              time_range: tuple = ("2024-01-01", "2024-03-31")) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            business_context: ä¸šåŠ¡èƒŒæ™¯
            time_range: æ—¶é—´èŒƒå›´
            
        Returns:
            åˆ†æç»“æœ
        """
        print("ğŸš€ å¼€å§‹å¤šæ™ºèƒ½ä½“åä½œåˆ†æ...")
        print(f"ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        print(f"æ—¶é—´èŒƒå›´: {time_range[0]} è‡³ {time_range[1]}")
        print("-" * 50)
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "user_query": user_query,
            "business_context": business_context,
            "time_range": time_range,
            "sql_query": "",
            "sql_explanation": "",
            "query_parameters": {},
            "raw_data": [],
            "data_summary": {},
            "data_quality_issues": [],
            "attribution_results": {},
            "key_findings": [],
            "recommendations": [],
            "current_agent": "",
            "execution_history": [],
            "errors": []
        }
        
        try:
            # æ‰§è¡Œå›¾
            result = self.graph.invoke(initial_state)
            
            # è®°å½•æ‰§è¡Œå†å²
            self.execution_history.append({
                "timestamp": datetime.now().isoformat(),
                "query": user_query,
                "agents_executed": len(result["execution_history"]),
                "has_data": len(result["raw_data"]) > 0
            })
            
            return self._format_results(result)
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            return {
                "error": str(e),
                "execution_state": "failed"
            }
    
    def _format_results(self, state: AnalysisState) -> Dict[str, Any]:
        """æ ¼å¼åŒ–åˆ†æç»“æœ"""
        return {
            "analysis_summary": {
                "query_executed": state["user_query"],
                "sql_generated": state["sql_query"][:200] + "..." if len(state["sql_query"]) > 200 else state["sql_query"],
                "data_points_analyzed": len(state["raw_data"]),
                "key_findings_count": len(state["key_findings"]),
                "recommendations_count": len(state["recommendations"])
            },
            "data_summary": state["data_summary"],
            "key_findings": state["key_findings"],
            "recommendations": state["recommendations"],
            "attribution_highlights": self._extract_attribution_highlights(state["attribution_results"]),
            "execution_details": {
                "agents_invoked": state["execution_history"],
                "data_quality_issues": state["data_quality_issues"],
                "final_agent": state["current_agent"]
            }
        }
    
    def _extract_attribution_highlights(self, attribution_results: Dict) -> Dict:
        """æå–å½’å› åˆ†æäº®ç‚¹"""
        highlights = {}
        
        # ç»´åº¦è´¡çŒ®åº¦
        dim_contrib = attribution_results.get("dimension_contribution", {})
        if "by_category" in dim_contrib:
            category_sales = dim_contrib["by_category"].get("sum", {})
            if category_sales:
                total = sum(category_sales.values())
                top_3 = sorted(category_sales.items(), key=lambda x: x[1], reverse=True)[:3]
                highlights["top_categories"] = [
                    {"category": cat, "sales": sales, "percentage": (sales/total)*100}
                    for cat, sales in top_3
                ]
        
        # å¼‚å¸¸æ£€æµ‹
        anomalies = attribution_results.get("anomaly_detection", {}).get("anomalies", [])
        if anomalies:
            highlights["anomalies_detected"] = len(anomalies)
            # è·å–æœ€å¤§çš„å¼‚å¸¸å€¼
            if anomalies:
                max_anomaly = max(anomalies, key=lambda x: abs(x.get('z_score', 0)))
                highlights["largest_anomaly"] = {
                    "value": max_anomaly.get('value'),
                    "z_score": max_anomaly.get('z_score'),
                    "date": max_anomaly.get('date')
                }
        
        return highlights

# ===================== ç¤ºä¾‹ä½¿ç”¨ =====================
def demonstrate_collaboration():
    """æ¼”ç¤ºå¤šæ™ºèƒ½ä½“åä½œ"""
    
    # åˆ›å»ºåä½œç³»ç»Ÿ
    collaborator = MultiAgentCollaborator()
    
    # ç¤ºä¾‹1: é”€å”®è¶‹åŠ¿åˆ†æ
    print("\nğŸ“Š ç¤ºä¾‹1: é”€å”®è¶‹åŠ¿åˆ†æ")
    print("=" * 60)
    
    result1 = collaborator.analyze_business_query(
        user_query="åˆ†æ2024å¹´ç¬¬ä¸€å­£åº¦å„äº§å“ç±»åˆ«çš„é”€å”®è¶‹åŠ¿å’Œè¡¨ç°",
        business_context="å…¬å¸è®¡åˆ’ä¼˜åŒ–äº§å“ç»„åˆå’ŒåŒºåŸŸç­–ç•¥",
        time_range=("2024-01-01", "2024-03-31")
    )
    
    print("\nğŸ“‹ åˆ†æç»“æœæ‘˜è¦:")
    print(f"- åˆ†ææ•°æ®ç‚¹: {result1['analysis_summary']['data_points_analyzed']}")
    print(f"- å…³é”®å‘ç°: {len(result1['key_findings'])} æ¡")
    print(f"- ä¸šåŠ¡å»ºè®®: {len(result1['recommendations'])} æ¡")
    
    print("\nğŸ”‘ å…³é”®å‘ç°:")
    for i, finding in enumerate(result1['key_findings'], 1):
        print(f"  {i}. {finding}")
    
    print("\nğŸ’¡ ä¸šåŠ¡å»ºè®®:")
    for i, recommendation in enumerate(result1['recommendations'][:5], 1):  # åªæ˜¾ç¤ºå‰5æ¡
        print(f"  {i}. {recommendation}")
    
    # ç¤ºä¾‹2: å¼‚å¸¸é”€å”®åˆ†æ
    print("\n\nğŸ” ç¤ºä¾‹2: å¼‚å¸¸é”€å”®æ£€æµ‹")
    print("=" * 60)
    
    result2 = collaborator.analyze_business_query(
        user_query="è¯†åˆ«è¿‘æœŸé”€å”®å¼‚å¸¸å¹¶åˆ†æåŸå› ",
        business_context="éœ€è¦ç›‘æ§é”€å”®æ³¢åŠ¨ï¼ŒåŠæ—¶è°ƒæ•´ç­–ç•¥",
        time_range=("2024-03-01", "2024-03-31")
    )
    
    if "attribution_highlights" in result2 and "anomalies_detected" in result2["attribution_highlights"]:
        anomalies_count = result2["attribution_highlights"]["anomalies_detected"]
        print(f"\næ£€æµ‹åˆ° {anomalies_count} ä¸ªé”€å”®å¼‚å¸¸ç‚¹")
        
        if "largest_anomaly" in result2["attribution_highlights"]:
            anomaly = result2["attribution_highlights"]["largest_anomaly"]
            print(f"æœ€å¤§å¼‚å¸¸å€¼: {anomaly.get('value', 0)} (Z-score: {anomaly.get('z_score', 0):.2f})")
    
    # æ˜¾ç¤ºæ‰§è¡Œå†å²
    print("\nğŸ“ˆ ç³»ç»Ÿæ‰§è¡Œå†å²:")
    for i, history in enumerate(collaborator.execution_history, 1):
        print(f"  æ‰§è¡Œ{i}: {history['query'][:50]}... | æ™ºèƒ½ä½“æ•°: {history['agents_executed']}")

# ===================== é«˜çº§åŠŸèƒ½ï¼šå¼‚æ­¥åä½œ =====================
class AsyncMultiAgentCollaborator:
    """å¼‚æ­¥å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿ"""
    
    def __init__(self):
        self.graph = create_collaboration_graph()
    
    async def analyze_streaming(self, user_query: str, callback=None):
        """æµå¼åˆ†æï¼Œå®æ—¶è¿”å›ç»“æœ"""
        
        initial_state = {
            "user_query": user_query,
            "business_context": "",
            "time_range": ("2024-01-01", "2024-03-31"),
            "sql_query": "",
            "raw_data": [],
            "current_agent": "",
            "execution_history": [],
            "errors": []
        }
        
        # æ¨¡æ‹Ÿæµå¼æ‰§è¡Œ
        agents = ["sql_generation", "query_execution", "attribution_analysis"]
        
        current_state = initial_state
        for agent_name in agents:
            print(f"\nâ¡ï¸ æ‰§è¡Œæ™ºèƒ½ä½“: {agent_name}")
            
            # æ‰§è¡Œå½“å‰æ™ºèƒ½ä½“
            current_state = await self._execute_agent_async(agent_name, current_state)
            
            # å›è°ƒå¤„ç†
            if callback:
                await callback({
                    "agent": agent_name,
                    "state": current_state,
                    "progress": agents.index(agent_name) / len(agents)
                })
            
            # æ·»åŠ å»¶è¿Ÿä»¥æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            await asyncio.sleep(1)
        
        return current_state
    
    async def _execute_agent_async(self, agent_name: str, state: AnalysisState) -> AnalysisState:
        """å¼‚æ­¥æ‰§è¡Œæ™ºèƒ½ä½“"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å¼‚æ­¥æ‰§è¡Œ
        # ç®€åŒ–ç¤ºä¾‹ï¼šç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•
        if agent_name == "sql_generation":
            sql_agent = SQLGeneratorAgent({})
            return sql_agent.generate_sql(state)
        elif agent_name == "query_execution":
            sql_agent = SQLGeneratorAgent({})
            return sql_agent.execute_query(state)
        elif agent_name == "attribution_analysis":
            attribution_agent = AttributionAnalysisAgent()
            return attribution_agent.analyze_attribution(state)
        return state

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    print("ğŸ¤ å¤šæ™ºèƒ½ä½“SQLç”Ÿæˆä¸å½’å› åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # æ¼”ç¤ºåŒæ­¥åä½œ
    demonstrate_collaboration()
    
    # æ¼”ç¤ºå¼‚æ­¥åä½œï¼ˆå¯é€‰ï¼‰
    async def demo_async():
        print("\n\nâš¡ å¼‚æ­¥åä½œæ¼”ç¤º")
        print("-" * 40)
        
        async def progress_callback(update):
            print(f"è¿›åº¦: {update['progress']*100:.0f}% - {update['agent']}")
        
        async_collaborator = AsyncMultiAgentCollaborator()
        result = await async_collaborator.analyze_streaming(
            "åˆ†æç”µå­äº§å“é”€å”®è¡¨ç°",
            callback=progress_callback
        )
        print(f"\nå¼‚æ­¥åˆ†æå®Œæˆï¼Œè·å– {len(result.get('raw_data', []))} æ¡æ•°æ®")
    
    # è¿è¡Œå¼‚æ­¥æ¼”ç¤º
    # asyncio.run(demo_async())
```

## ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ç”¨æˆ·æŸ¥è¯¢                                  â”‚
â”‚                "åˆ†æé”€å”®è¶‹åŠ¿"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               å¤šæ™ºèƒ½ä½“åä½œå›¾ (StateGraph)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ™ºèƒ½ä½“1      â”‚  æ™ºèƒ½ä½“1      â”‚      æ™ºèƒ½ä½“2                â”‚
â”‚  SQLç”Ÿæˆ      â”‚ æ•°æ®æŸ¥è¯¢      â”‚     å½’å› åˆ†æ                â”‚
â”‚              â”‚              â”‚                              â”‚
â”‚  â€¢ è§£ææ„å›¾   â”‚  â€¢ æ‰§è¡ŒæŸ¥è¯¢   â”‚  â€¢ å¤šç»´å½’å›                  â”‚
â”‚  â€¢ ç”ŸæˆSQL    â”‚  â€¢ è´¨é‡æ£€æŸ¥   â”‚  â€¢ è¶‹åŠ¿åˆ†æ                â”‚
â”‚  â€¢ å‚æ•°æå–   â”‚  â€¢ æ•°æ®æ‘˜è¦   â”‚  â€¢ å¼‚å¸¸æ£€æµ‹                â”‚
â”‚              â”‚              â”‚  â€¢ ä¸šåŠ¡å»ºè®®                 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚                      â”‚
       â”‚   SQLæŸ¥è¯¢     â”‚   åŸå§‹æ•°æ®           â”‚  åˆ†æç»“æœ
       â–¼               â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å…±äº«çŠ¶æ€ (AnalysisState)                  â”‚
â”‚                                                            â”‚
â”‚  user_query: "åˆ†æé”€å”®è¶‹åŠ¿"                                â”‚
â”‚  sql_query: "SELECT ..."                                   â”‚
â”‚  raw_data: [{...}, {...}, ...]                             â”‚
â”‚  attribution_results: {trends: {}, anomalies: {}}          â”‚
â”‚  key_findings: ["ç”µå­äº§å“é”€å”®å¢é•¿æœ€å¿«"]                    â”‚
â”‚  recommendations: ["åŠ å¤§ç”µå­äº§å“è¥é”€"]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å…³é”®ç‰¹æ€§

### 1. **æ™ºèƒ½ä½“é—´æ•°æ®ä¼ é€’æœºåˆ¶**
```python
# æ™ºèƒ½ä½“1 â†’ æ™ºèƒ½ä½“2 çš„æ•°æ®ä¼ é€’
state = {
    # SQLæ™ºèƒ½ä½“å†™å…¥
    "sql_query": "SELECT ...",
    "raw_data": [...],
    
    # å½’å› æ™ºèƒ½ä½“è¯»å–å¹¶å¤„ç†
    "attribution_results": {...},
    
    # åŒå‘é€šä¿¡
    "execution_history": ["SQLç”Ÿæˆå®Œæˆ", "æ•°æ®æŸ¥è¯¢å®Œæˆ", "å½’å› åˆ†æå®Œæˆ"]
}
```

### 2. **çŠ¶æ€é©±åŠ¨åä½œ**
- **ç»Ÿä¸€çŠ¶æ€ç®¡ç†**ï¼šæ‰€æœ‰æ™ºèƒ½ä½“å…±äº«åŒä¸€çŠ¶æ€å¯¹è±¡
- **æ•°æ®å®Œæ•´æ€§**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“åªä¿®æ”¹è‡ªå·±è´Ÿè´£çš„éƒ¨åˆ†
- **æ‰§è¡Œè¿½è¸ª**ï¼šå®Œæ•´è®°å½•æ¯ä¸ªæ™ºèƒ½ä½“çš„æ‰§è¡Œå†å²

### 3. **é”™è¯¯å¤„ç†ä¸è´¨é‡æ£€æŸ¥**
```python
# æ•°æ®è´¨é‡æ£€æŸ¥
data_quality_issues = [
    "ç¬¬5è¡Œåˆ—regionå­˜åœ¨ç©ºå€¼",
    "æ•°æ®ç±»åˆ«å•ä¸€ï¼Œå¯èƒ½å½±å“åˆ†æç»“æœ"
]

# æ¡ä»¶è·¯ç”±ï¼šåŸºäºè´¨é‡å†³å®šæ˜¯å¦ç»§ç»­
if len(data_quality_issues) > 3:
    return "needs_data_cleanup"  # è½¬å‘æ•°æ®æ¸…æ´—
else:
    return "attribution_analysis"  # ç»§ç»­åˆ†æ
```

### 4. **å¯æ‰©å±•æ¶æ„**
```python
# å¯ä»¥è½»æ¾æ·»åŠ æ–°æ™ºèƒ½ä½“
graph.add_node("data_visualization", visualization_agent.generate_charts)
graph.add_node("report_generation", report_agent.generate_report)

# ä¿®æ”¹æ‰§è¡Œæµç¨‹
graph.add_edge("attribution_analysis", "data_visualization")
graph.add_edge("data_visualization", "report_generation")
```

## å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯1ï¼šç”µå•†é”€å”®åˆ†æ
```python
collaborator = MultiAgentCollaborator()
result = collaborator.analyze_business_query(
    user_query="åˆ†æåŒåä¸€æœŸé—´å„å“ç±»é”€å”®è¡¨ç°ï¼Œæ‰¾å‡ºå¢é•¿é©±åŠ¨å› ç´ ",
    business_context="å‡†å¤‡æ˜å¹´åŒåä¸€è¥é”€ç­–ç•¥",
    time_range=("2023-11-01", "2023-11-15")
)
```

### åœºæ™¯2ï¼šé‡‘èé£é™©ç›‘æ§
```python
result = collaborator.analyze_business_query(
    user_query="æ£€æµ‹å¼‚å¸¸äº¤æ˜“æ¨¡å¼ï¼Œåˆ†æé£é™©å› ç´ ",
    business_context="åæ´—é’±ç›‘æ§ç³»ç»Ÿ",
    time_range=("2024-01-01", "2024-01-31")
)
```

### åœºæ™¯3ï¼šç”Ÿäº§è´¨é‡åˆ†æ
```python
result = collaborator.analyze_business_query(
    user_query="åˆ†æäº§å“è´¨é‡ç¼ºé™·çš„åŸå› ï¼Œè¯†åˆ«å…³é”®å½±å“å› ç´ ",
    business_context="æå‡äº§å“è´¨é‡ï¼Œé™ä½é€€è´§ç‡",
    time_range=("2024-02-01", "2024-02-29")
)
```

è¿™ä¸ªç³»ç»Ÿå±•ç¤ºäº†å¦‚ä½•é€šè¿‡LangGraphå®ç°æ™ºèƒ½ä½“é—´çš„æœ‰æ•ˆåä½œï¼Œå…¶ä¸­ï¼š
1. **SQLç”Ÿæˆæ™ºèƒ½ä½“**è´Ÿè´£ç†è§£éœ€æ±‚å¹¶è·å–æ•°æ®
2. **å½’å› åˆ†ææ™ºèƒ½ä½“**è´Ÿè´£æ·±åº¦åˆ†æå’Œæ´å¯Ÿå‘ç°
3. **å…±äº«çŠ¶æ€**ä½œä¸ºé€šä¿¡æ¡¥æ¢ï¼Œç¡®ä¿æ•°æ®å®Œæ•´ä¼ é€’
4. **æ¡ä»¶è·¯ç”±**å®ç°æ™ºèƒ½çš„é”™è¯¯å¤„ç†å’Œæµç¨‹æ§åˆ¶

è¿™ç§è®¾è®¡æ¨¡å¼å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ·»åŠ æ›´å¤šä¸“ä¸šæ™ºèƒ½ä½“ï¼ˆå¦‚é¢„æµ‹æ™ºèƒ½ä½“ã€ä¼˜åŒ–æ™ºèƒ½ä½“ç­‰ï¼‰ï¼Œæ„å»ºå®Œæ•´çš„æ•°æ®åˆ†ææµæ°´çº¿ã€‚